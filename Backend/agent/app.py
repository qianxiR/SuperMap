"""
Agent Service - FastAPI entry for LLM chat proxy with full LLM management features

Usage (dev):
  python -m uvicorn agent.app:app --reload --host 0.0.0.0 --port 8089
"""
from fastapi import FastAPI, APIRouter, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Literal, Optional, Dict, Any
import httpx
import uvicorn
import os
from pathlib import Path
from dotenv import load_dotenv


class LLMSettings(BaseModel):
    api_key: str = Field(default_factory=lambda: os.getenv("DASHSCOPE_API_KEY", ""))
    base_url: str = Field(default_factory=lambda: os.getenv("DASHSCOPE_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1"))
    model: str = Field(default_factory=lambda: os.getenv("DASHSCOPE_MODEL", "qwen-plus"))
    temperature: float = Field(default_factory=lambda: float(os.getenv("DASHSCOPE_TEMPERATURE", "0.7")))
    max_tokens: int = Field(default_factory=lambda: int(os.getenv("DASHSCOPE_MAX_TOKENS", "3000")))
    cors_origins: str = Field(default_factory=lambda: os.getenv("CORS_ORIGINS", "*"))

    def cors_list(self) -> List[str]:
        raw = self.cors_origins or "*"
        if raw == "*":
            return ["*"]
        return [o.strip() for o in raw.split(",")]


# åŠ è½½åç«¯ç¯å¢ƒå˜é‡æ–‡ä»¶ Backend/.env
_ROOT = Path(__file__).resolve().parents[1]
_ENV_PATH = _ROOT / ".env"
if _ENV_PATH.exists():
    load_dotenv(dotenv_path=str(_ENV_PATH))

settings = LLMSettings()


class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant", "tool"]
    content: str
    name: Optional[str] = None


class ChatRequest(BaseModel):
    api_key: str | None = None
    base_url: str | None = None
    apiKey: str | None = None
    baseUrl: str | None = None
    model: str
    temperature: float
    max_tokens: int | None = None
    maxTokens: int | None = None
    stream: bool
    messages: List[ChatMessage]


class ChatResponse(BaseModel):
    success: bool
    data: Optional[Dict[str, Any]] = None
    error: Optional[str] = None


def load_system_prompt() -> str:
    """åŠ è½½ç³»ç»Ÿæç¤ºè¯ï¼Œä¼˜å…ˆä»prompt/prompt.mdè¯»å–"""
    base = Path(__file__).resolve().parent
    prompt_path = base / "prompt" / "prompt.md"
    if not prompt_path.exists():
        prompt_path = base / "prompt.md"
    try:
        content = prompt_path.read_text(encoding="utf-8")
        print(f"âœ… æˆåŠŸåŠ è½½ç³»ç»Ÿæç¤ºè¯: {prompt_path}")
        return content
    except Exception as e:
        print(f"âš ï¸ åŠ è½½ç³»ç»Ÿæç¤ºè¯å¤±è´¥: {e}")
        return "You are a helpful spatial analysis assistant."


router = APIRouter(prefix="/agent", tags=["agent"])


@router.post("/chat", response_model=ChatResponse)
async def chat(req: ChatRequest):
    """
    èŠå¤©æ¥å£ - è‡ªåŠ¨æ³¨å…¥prompt.mdä¸­çš„ç³»ç»Ÿæç¤ºè¯
    """
    # åŠ è½½å¹¶æ³¨å…¥ç³»ç»Ÿæç¤ºè¯
    sys_prompt = load_system_prompt()
    messages = req.messages
    
    # ç¡®ä¿ç³»ç»Ÿæç¤ºè¯åœ¨æœ€å‰é¢
    payload_messages = [{"role": "system", "content": sys_prompt}]
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯å†å²ï¼ˆè¿‡æ»¤æ‰å·²æœ‰çš„systemæ¶ˆæ¯ï¼Œé¿å…é‡å¤ï¼‰
    for msg in messages:
        if msg.role != "system":
            payload_messages.append(msg.model_dump())

    # æ”¯æŒé©¼å³°/ä¸‹åˆ’çº¿å‚æ•°å
    api_key = req.api_key or req.apiKey or ""
    base_url = (req.base_url or req.baseUrl or "").rstrip("/")
    max_tokens = req.max_tokens if req.max_tokens is not None else (req.maxTokens if req.maxTokens is not None else settings.max_tokens)

    body = {
        "model": req.model,
        "temperature": req.temperature,
        "max_tokens": max_tokens,
        "messages": payload_messages,
        "stream": req.stream
    }

    url = base_url + "/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    print(f"ğŸ¤– å‘é€èŠå¤©è¯·æ±‚åˆ°: {url}")
    print(f"ğŸ“ ç³»ç»Ÿæç¤ºè¯å·²æ³¨å…¥ï¼Œæ¶ˆæ¯æ•°é‡: {len(payload_messages)}")

    async with httpx.AsyncClient(timeout=60.0, verify=False) as client:
        resp = await client.post(url, headers=headers, json=body)
        if resp.status_code != 200:
            print(f"âŒ APIè°ƒç”¨å¤±è´¥: {resp.status_code} - {resp.text}")
            raise HTTPException(status_code=resp.status_code, detail=resp.text)
        data = resp.json()
        print(f"âœ… APIè°ƒç”¨æˆåŠŸ")
        return ChatResponse(success=True, data=data)


app = FastAPI(
    title="Agent Service", 
    version="2.0.0",
    description="LLM AgentæœåŠ¡ - æä¾›å®Œæ•´çš„AIåŠ©æ‰‹ç®¡ç†åŠŸèƒ½"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_list(),
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(router)


@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {
        "status": "ok",
        "service": "Agent Service",
        "version": "2.0.0",
        "features": [
            "LLM Chat with System Prompt Injection",
            "API Key Management", 
            "Prompt Template Management",
            "Knowledge Base Management"
        ],
        "prompt_loaded": load_system_prompt() != "You are a helpful spatial analysis assistant."
    }


@app.get("/")
async def root():
    """æ ¹è·¯å¾„ä¿¡æ¯"""
    return {
        "message": "Agent Service API",
        "docs": "/docs",
        "health": "/health",
        "endpoints": {
            "chat": "/agent/chat",
            "api_keys": "/api/v1/api-keys",
            "prompts": "/api/v1/prompts", 
            "knowledge": "/api/v1/knowledge"
        }
    }



if __name__ == "__main__":
    uvicorn.run("agent.app:app", host="0.0.0.0", port=8089, reload=True)