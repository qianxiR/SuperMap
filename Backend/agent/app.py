"""
Agent Service - FastAPI entry for LLM chat proxy with full LLM management features

Usage (dev):
  python -m uvicorn agent.app:app --reload --host 0.0.0.0 --port 8089
"""
from fastapi import FastAPI, APIRouter, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Literal, Optional, Dict, Any
import httpx
import uvicorn
import os
from pathlib import Path
from dotenv import load_dotenv
from typing import Set


class LLMSettings(BaseModel):
    api_key: str = Field(default_factory=lambda: os.getenv("DASHSCOPE_API_KEY", ""))
    base_url: str = Field(default_factory=lambda: os.getenv("DASHSCOPE_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1"))
    model: str = Field(default_factory=lambda: os.getenv("DASHSCOPE_MODEL", "qwen-plus"))
    temperature: float = Field(default_factory=lambda: float(os.getenv("DASHSCOPE_TEMPERATURE", "0.7")))
    max_tokens: int = Field(default_factory=lambda: int(os.getenv("DASHSCOPE_MAX_TOKENS", "3000")))
    cors_origins: str = Field(default_factory=lambda: os.getenv("CORS_ORIGINS", "*"))

    def cors_list(self) -> List[str]:
        raw = self.cors_origins or "*"
        if raw == "*":
            return ["*"]
        return [o.strip() for o in raw.split(",")]


# åŠ è½½åç«¯ç¯å¢ƒå˜é‡æ–‡ä»¶ Backend/.env
_ROOT = Path(__file__).resolve().parents[1]
_ENV_PATH = _ROOT / ".env"
if _ENV_PATH.exists():
    load_dotenv(dotenv_path=str(_ENV_PATH))

settings = LLMSettings()


class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant", "tool"]
    content: str
    name: Optional[str] = None


class ChatRequest(BaseModel):
    api_key: str | None = None
    base_url: str | None = None
    apiKey: str | None = None
    baseUrl: str | None = None
    model: str
    temperature: float
    max_tokens: int | None = None
    maxTokens: int | None = None
    stream: bool
    conversation_id: str | None = None
    conversationId: str | None = None
    messages: List[ChatMessage]


class ChatResponse(BaseModel):
    success: bool
    data: Optional[Dict[str, Any]] = None
    error: Optional[str] = None


def load_system_prompt() -> str:
    """åŠ è½½ç³»ç»Ÿæç¤ºè¯ï¼Œä¼˜å…ˆä»prompt/tools.mdè¯»å–"""
    base = Path(__file__).resolve().parent
    prompt_path = base / "prompt" / "tools.md"
    if not prompt_path.exists():
        prompt_path = base / "tools.md"
    try:
        content = prompt_path.read_text(encoding="utf-8")
        print(f"âœ… æˆåŠŸåŠ è½½ç³»ç»Ÿæç¤ºè¯: {prompt_path}")
        return content
    except Exception as e:
        print(f"âš ï¸ åŠ è½½ç³»ç»Ÿæç¤ºè¯å¤±è´¥: {e}")
        return "You are a helpful spatial analysis assistant."

def load_tools_prompt() -> str:
    base = Path(__file__).resolve().parent
    # ä¼˜å…ˆä½¿ç”¨ç‹¬ç«‹å·¥å…·æç¤ºè¯æ–‡ä»¶ agent/tools/tools.md
    tools_path = base / "tools" / "tools.md"
    if not tools_path.exists():
        # å›é€€åˆ° prompt/tools.mdï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        tools_path = base / "prompt" / "tools.md"
    try:
        return tools_path.read_text(encoding="utf-8")
    except Exception:
        return ""


router = APIRouter(prefix="/agent", tags=["agent"])

# è®°å½•å·²æ³¨å…¥è¿‡ç³»ç»Ÿæç¤ºè¯çš„ä¼šè¯
_injected_conversations: Set[str] = set()


@router.post("/chat", response_model=ChatResponse)
async def chat(req: ChatRequest):
    """
    èŠå¤©æ¥å£ - è‡ªåŠ¨æ³¨å…¥main.mdä¸­çš„ç³»ç»Ÿæç¤ºè¯
    """
    # å¤„ç†ä¼šè¯IDï¼Œå¹¶æŒ‰éœ€æ³¨å…¥ç³»ç»Ÿæç¤ºè¯
    conv_id = req.conversation_id or req.conversationId or ""
    messages = req.messages
    payload_messages: List[Dict[str, Any]] = []
    # ä¸ºç¡®ä¿ç³»ç»Ÿæç¤ºè¯ç¨³å®šç”Ÿæ•ˆï¼šæ¯æ¬¡è¯·æ±‚éƒ½æ³¨å…¥
    should_inject = True
    if should_inject:
        sys_prompt = load_system_prompt()
        tools_prompt = load_tools_prompt()
        payload_messages.append({"role": "system", "content": sys_prompt})
        if tools_prompt:
            payload_messages.append({"role": "system", "content": tools_prompt})
    for msg in messages:
        if msg.role != "system":
            payload_messages.append(msg.model_dump())

    # æ”¯æŒé©¼å³°/ä¸‹åˆ’çº¿å‚æ•°å
    api_key = req.api_key or req.apiKey or settings.api_key
    base_url = (req.base_url or req.baseUrl or settings.base_url).rstrip("/")
    max_tokens = req.max_tokens if req.max_tokens is not None else (req.maxTokens if req.maxTokens is not None else settings.max_tokens)
    
    # éªŒè¯å¿…è¦å‚æ•°
    if not api_key:
        raise HTTPException(status_code=400, detail="APIå¯†é’¥æœªé…ç½®")
    if not base_url:
        raise HTTPException(status_code=400, detail="APIåœ°å€æœªé…ç½®")
    if not base_url.startswith(('http://', 'https://')):
        raise HTTPException(status_code=400, detail=f"APIåœ°å€æ ¼å¼é”™è¯¯: {base_url}")


    body = {
        "model": req.model,
        "temperature": req.temperature,
        "max_tokens": max_tokens,
        "messages": payload_messages,
        "stream": req.stream
    }

    url = base_url + "/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    print(f"ğŸ¤– å‘é€èŠå¤©è¯·æ±‚åˆ°: {url}")
    print(f"ğŸ“ ç³»ç»Ÿæç¤ºè¯å·²æ³¨å…¥ï¼Œæ¶ˆæ¯æ•°é‡: {len(payload_messages)}")
    print(f"ğŸ”‘ APIå¯†é’¥: {api_key[:10]}..." if len(api_key) > 10 else f"ğŸ”‘ APIå¯†é’¥: {api_key}")
    print(f"ğŸŒ APIåœ°å€: {base_url}")

    try:
        async with httpx.AsyncClient(timeout=60.0, verify=False) as client:
            resp = await client.post(url, headers=headers, json=body)
            if resp.status_code != 200:
                print(f"âŒ APIè°ƒç”¨å¤±è´¥: {resp.status_code} - {resp.text}")
                raise HTTPException(status_code=resp.status_code, detail=resp.text)
            data = resp.json()
    except httpx.ConnectError as e:
        print(f"âŒ è¿æ¥å¤±è´¥: {str(e)}")
        print(f"âŒ ç›®æ ‡URL: {url}")
        raise HTTPException(status_code=500, detail=f"æ— æ³•è¿æ¥åˆ°LLMæœåŠ¡: {base_url}")
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")
        raise HTTPException(status_code=500, detail=f"LLMè¯·æ±‚å¼‚å¸¸: {str(e)}")


    print(f"âœ… APIè°ƒç”¨æˆåŠŸ")
    return ChatResponse(success=True, data=data)


app = FastAPI(
    title="Agent Service", 
    version="2.0.0",
    description="LLM AgentæœåŠ¡ - æä¾›å®Œæ•´çš„AIåŠ©æ‰‹ç®¡ç†åŠŸèƒ½"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_list(),
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(router)


@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {
        "status": "ok",
        "service": "Agent Service",
        "version": "2.0.0",
        "features": [
            "LLM Chat with System Prompt Injection",
            "API Key Management", 
            "Prompt Template Management",
            "Knowledge Base Management"
        ],
        "prompt_loaded": load_system_prompt() != "You are a helpful spatial analysis assistant."
    }


@app.get("/")
async def root():
    """æ ¹è·¯å¾„ä¿¡æ¯"""
    return {
        "message": "Agent Service API",
        "docs": "/docs",
        "health": "/health",
        "endpoints": {
            "chat": "/agent/chat",
            "api_keys": "/api/v1/api-keys",
            "prompts": "/api/v1/prompts", 
            "knowledge": "/api/v1/knowledge"
        }
    }



if __name__ == "__main__":
    uvicorn.run("agent.app:app", host="0.0.0.0", port=8089, reload=True)